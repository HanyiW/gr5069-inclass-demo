## TOPIC 3 - Structuring your workspace: Data Science & Data Engineering Perspectives

#### In preparation for this week:
* read/listen/watch as much as you can from the annotated materials below

---

A Data Science Shop must follow three general principles to be equipped for collaboration:

* **replicability**
* **portability**
* **scalability**

We'll discuss some principles - and tricks - that take your workflow to that level.

### Structuring the Data Science workspace

People have put thought into standardizing workflows in Data Science starting with what workspaces should look like. Two very useful templates for workspaces are [Cookiecutter for Data Science](http://drivendata.github.io/cookiecutter-data-science/) tailored for Python, and [ProjectTemplate](http://projecttemplate.net/index.html) developed for R. They are a rich source of food for thought on what needs to be considered when building your workspace in a Data Science Shop.

### Structuring the Data Science workflow

We will cover many current best practices in the field throughout the course. But there is one which still stirs passions: __Agile__. Some swear by it, while others swear it's not a great idea for Data Science work.  (Not surprisingly, one of us is a big believer in the Agile way, but the other is a big believer in only implementing Agile-like principles when it makes sense). The jury is still out on __Agile__, but precisely for that reason you should understand what it is and what makes it attractive in the field. A few recommended posts will help you get there.

Start by reading [__The Psychology of Agile__](https://medium.com/agile-in-learning/the-psychology-of-agile-87f92521a5ed) that summarizes the benefits that people perceive from the **Agile** way. Read also [__Stop Brainstorming and Start Sprinting__](https://medium.com/@jakek/stop-brainstorming-and-start-sprinting-16180839b43d) about what a **sprint** is, how it works, and why it's useful, but don't forget to read a complementary perspective in [__From Agile to Fragile: How to unravel your team in one sprint__](https://medium.com/agile-in-learning/from-agile-to-fragile-how-to-unravel-your-team-in-one-sprint-5e60e70a557) on how and where sprints can go wrong. An incredibly helpful reading is [**Don’t Make Data Scientists Do Scrum**](https://towardsdatascience.com/dont-make-data-scientists-do-scrum-de87bc921a6b) that makes a seasoned and compelling argument explaining why it may not always be the path to follow for Data Science. For a more formal argument, read Kery & Meyers' (2017) article - [__Exploring Exploratory Programming__](https://marybethkery.com/projects/Verdant/ExploringExploratoryProgramming.pdf) - that details the kind of programming activities that are common to the work of Data Scientists and engineers working with data. Wrap up with [__MVP Paradox And Here’s How To Fix Your MVP Before Its Too Late!__](https://hackernoon.com/mvp-paradox-and-what-most-founders-need-to-be-aware-of-3a5f8c3acb76) on how to think about minimum viable products (**MVPs**).

### Cloud computing for our live workshops

We will be using "the Cloud" for the majority of our collaborative work in this course, including our live in-class workshops. It is important to understand the basics and meaning of **Cloud Computing**. This [Medium Article](https://towardsdatascience.com/aws-and-cloud-computing-for-dummies-84525fbabd1e) talks about the traditional IT Infrastructure, types of cloud services and benefits of the cloud.

#### An Introduction to Apache Spark

What is [**Apache Spark**](https://spark.apache.org)? - Many organizations have adapted to using Apache Spark for large scale data processing and machine learning models. Apache Spark has been proven to provide speed, efficiency and reliability in many use cases. In this course we will be using [**Databricks**](https://databricks.com) which is a managed Apache Spark Platform that allows the management of Apache Spark easy. In addition, Databricks provides a very collaborative platform for both Data Scientists and Data Engineers to build data pipelines and productionalize machine learning models very easily. Skim as much as you can from the materials below (it will pay off later in the class):

* [**Intro to Apache Spark**](https://www.youtube.com/watch?v=9U4ED7KQwlE&t=22s): a 60-min video to provide an easy to digest introduction to Spark for all audiences
* [**"An Architecture for Fast and General Data Processing on Large Clusters"**](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf): Mattei Zaharia's doctoral dissertation to get a great theoretical introduction to Spark
* [**Learning Spark, 2nd edition**](https://pages.databricks.com/rs/094-YMS-629/images/LearningSpark2.0.pdf): a book-length introduction to Spark - from zero to Spark in 12 chapters!
* free [**self-paced learning courses**](https://docs.google.com/document/d/14YSH67RYaIcgHbgxs-MaDOjpWWGEfskkmDFIOREiRDs/edit) from [Databricks](https://databricks.com) for university students
* [Databricks notebook gallery](https://databricks.com/discover/notebook-gallery) featuring sample notebooks for a large range of use cases
* [Apache Spark GitHub Repo](https://github.com/apache/spark)
* [Research Papers](https://spark.apache.org/research.html)
